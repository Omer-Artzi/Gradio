# -*- coding: utf-8 -*-
"""gradio_app_OY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/105gt_7vLC9bpD-P6gNV26cMdwDnBhGtG
"""

import torch
import torchvision
import numpy as np
import os
from omegaconf import OmegaConf
from PIL import Image
from utils.app_utils import (
    remove_background,
    resize_foreground,
    set_white_background,
    resize_to_128,
    to_tensor,
    get_source_camera_v2w_rmo_and_quats,
    get_target_cameras,
    export_to_obj)
import imageio
from scene.gaussian_predictor import GaussianSplatPredictor
from gaussian_renderer import render_predicted
import gradio as gr
import matplotlib.pyplot as plt
import rembg
from huggingface_hub import hf_hub_download

# Set matplotlib to use the 'Agg' backend for environments without GUI
import matplotlib
matplotlib.use('Agg')

@torch.no_grad()
def main():
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model_cfg = OmegaConf.load("gradio_config.yaml")

    model_path = hf_hub_download(repo_id="szymanowiczs/splatter-image-multi-category-v1",
                                filename="model_latest.pth")

    model = GaussianSplatPredictor(model_cfg)
    ckpt_loaded = torch.load(model_path, map_location=device)
    model.load_state_dict(ckpt_loaded["model_state_dict"])
    model.to(device)

    rembg_session = rembg.new_session()

    def check_input_image(input_image):
        if input_image is None:
            raise gr.Error("No image uploaded!")

    def preprocess(input_image, preprocess_background=True, foreground_ratio=0.65):
        if preprocess_background:
            image = input_image.convert("RGB")
            image = remove_background(image, rembg_session)
            image = resize_foreground(image, foreground_ratio)
            image = set_white_background(image)
        else:
            image = input_image
            if image.mode == "RGBA":
                image = set_white_background(image)
        image = resize_to_128(image)
        return image

    ply_out_path = './mesh.ply'

def visualize_parameters(opacity, rgb, sigma, xyz):
    print("Saving Opacity Image...") if opacity is not None else print("No Opacity Data")
    if opacity is not None:
        plt.figure()
        plt.imshow(opacity.cpu().detach().numpy(), cmap='gray')
        plt.title('Opacity')
        plt.colorbar()
        plt.savefig('opacity.png')
        plt.close()

    print("Saving RGB Image...") if rgb is not None else print("No RGB Data")
    if rgb is not None:
        plt.figure()
        plt.imshow(rgb.cpu().detach().numpy())
        plt.title('RGB')
        plt.savefig('rgb.png')
        plt.close()

    print("Saving Sigma Image...") if sigma is not None else print("No Sigma Data")
    if sigma is not None:
        plt.figure()
        plt.imshow(sigma.cpu().detach().numpy(), cmap='viridis')
        plt.title('Sigma')
        plt.colorbar()
        plt.savefig('sigma.png')
        plt.close()

    print("Saving XYZ Images...") if xyz is not None else print("No XYZ Data")
    if xyz is not None:
        fig, axs = plt.subplots(1, 3, figsize=(15, 5))
        axs[0].imshow(xyz[0].cpu().detach().numpy(), cmap='jet')
        axs[0].set_title('X')
        axs[1].imshow(xyz[1].cpu().detach().numpy(), cmap='jet')
        axs[1].set_title('Y')
        axs[2].imshow(xyz[2].cpu().detach().numpy(), cmap='jet')
        axs[2].set_title('Z')
        plt.savefig('xyz.png')
        plt.close()


    def reconstruct_and_export(image):
        image = to_tensor(image).to(device)
        view_to_world_source, rot_transform_quats = get_source_camera_v2w_rmo_and_quats()
        view_to_world_source = view_to_world_source.to(device)
        rot_transform_quats = rot_transform_quats.to(device)

        reconstruction_unactivated = model(
            image.unsqueeze(0).unsqueeze(0),
            view_to_world_source,
            rot_transform_quats,
            None,
            activate_output=False)

        reconstruction = {k: v[0].contiguous() for k, v in reconstruction_unactivated.items()}
        reconstruction["scaling"] = model.scaling_activation(reconstruction["scaling"])
        reconstruction["opacity"] = model.opacity_activation(reconstruction["opacity"])

        # Extract and visualize parameters
        opacity = reconstruction.get("opacity", None)
        rgb = reconstruction.get("rgb", None)
        sigma = reconstruction.get("scaling", None)
        xyz = reconstruction.get("xyz", None)
        visualize_parameters(opacity, rgb, sigma, xyz)

        # Render loop and export to .ply
        world_view_transforms, full_proj_transforms, camera_centers = get_target_cameras()
        background = torch.tensor([1, 1, 1], dtype=torch.float32, device=device)
        loop_renders = []
        t_to_512 = torchvision.transforms.Resize(512, interpolation=torchvision.transforms.InterpolationMode.NEAREST)
        for r_idx in range(world_view_transforms.shape[0]):
            image = render_predicted(reconstruction,
                                     world_view_transforms[r_idx].to(device),
                                     full_proj_transforms[r_idx].to(device),
                                     camera_centers[r_idx].to(device),
                                     background,
                                     model_cfg,
                                     focals_pixels=None)["render"]
            image = t_to_512(image)
            loop_renders.append(torch.clamp(image * 255, 0.0, 255.0).detach().permute(1, 2, 0).cpu().numpy().astype(np.uint8))
        loop_out_path = os.path.join(os.path.dirname(ply_out_path), "loop.mp4")
        imageio.mimsave(loop_out_path, loop_renders, fps=25)
        export_to_obj(reconstruction_unactivated, ply_out_path)

        return ply_out_path, loop_out_path

    css = """
    h1 {
        text-align: center;
        display:block;
    }
    """

    def run_example(image):
        preprocessed = preprocess(image)
        ply_out_path, loop_out_path = reconstruct_and_export(np.array(preprocessed))
        return preprocessed, ply_out_path, loop_out_path

    with gr.Blocks(css=css) as demo:
        gr.Markdown(
            """
            # Splatter Image Demo (CVPR 2024)
            """
        )
        with gr.Row(variant="panel"):
            with gr.Column():
                input_image = gr.Image(
                    label="Input Image",
                    image_mode="RGBA",
                    type="pil",
                    elem_id="content_image"
                )
                processed_image = gr.Image(label="Processed Image", interactive=False)
                preprocess_background = gr.Checkbox(label="Remove Background", value=True)
                submit = gr.Button("Generate", elem_id="generate", variant="primary")
            with gr.Column():
                with gr.Tab("Reconstruction"):
                    output_video = gr.Video(value=None, width=512, label="Rendered Video", autoplay=True)
                    output_model = gr.Model3D(
                        height=512,
                        label="Output Model",
                        interactive=False
                    )
                with gr.Tab("Visualizations"):
                    opacity_image = gr.Image(label="Opacity Visualization")
                    rgb_image = gr.Image(label="RGB Visualization")
                    sigma_image = gr.Image(label="Sigma Visualization")
                    xyz_image = gr.Image(label="XYZ Visualization")

        submit.click(fn=check_input_image, inputs=[input_image]).success(
            fn=preprocess,
            inputs=[input_image, preprocess_background],
            outputs=[processed_image],
        ).success(
            fn=reconstruct_and_export,
            inputs=[processed_image],
            outputs=[output_model, output_video],
        )

    demo.queue(max_size=1)
    demo.launch(share=True)

main()